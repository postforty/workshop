# GPT (Generative Pre-trained Transformer)

## 개요
GPT는 **생성형 사전 학습 트랜스포머**(Generative Pre-trained Transformer)의 약자로, 현대 LLM의 가장 대표적인 아키텍처입니다. OpenAI의 ChatGPT, Google의 Gemini, Anthropic의 Claude 등 대부분의 주요 LLM이 이 구조를 기반으로 합니다.

## GPT의 세 가지 핵심 요소

### 1. Generative (생성형)
**새로운 텍스트를 생성**하는 능력을 의미합니다.
- 단순히 기존 텍스트를 검색하거나 복사하는 것이 아님
- 학습한 패턴을 바탕으로 **창의적인 새 문장**을 만들어냄
- 예: 질문에 대한 답변, 이야기 창작, 코드 작성 등

### 2. Pre-trained (사전 학습)
모델이 **특정 작업 전에 미리 대규모 데이터로 학습**되었음을 의미합니다.

**2단계 학습 과정:**

#### 1단계: 사전 학습 (Pre-training)
- 인터넷의 방대한 텍스트 데이터로 언어의 일반적인 패턴 학습
- "다음 단어 예측" 과제를 반복 수행
- 이 단계에서 언어의 문법, 상식, 지식 등을 습득

#### 2단계: 미세 조정 (Fine-tuning)
- 특정 목적에 맞게 추가 학습
- 예: 대화형 AI, 코드 생성, 번역 등
- 사람의 피드백을 반영하여 더 유용한 답변 생성 (RLHF: Reinforcement Learning from Human Feedback)

### 3. Transformer (트랜스포머)
GPT의 핵심 기술 구조로, **문맥을 효율적으로 이해**하는 메커니즘입니다.

#### 주요 특징: Attention Mechanism (어텐션 메커니즘)
문장 내 단어들 간의 관계를 파악하여 문맥을 이해합니다.

**예시:**
```
문장: "은행에 가서 돈을 찾았다"
```

Transformer는 "은행"이라는 단어를 볼 때:
- "돈"이라는 단어와의 관계를 파악 → 금융기관의 "은행"으로 이해
- 만약 "강"이라는 단어가 있었다면 → 강가의 "은행(bank)"으로 이해

이처럼 **주변 단어들에 주목(attention)**하여 문맥에 맞는 의미를 파악합니다.

## GPT의 발전 과정

| 모델 | 출시 연도 | 파라미터 수 | 주요 특징 |
|------|----------|------------|----------|
| **GPT-1** | 2018 | 1.17억 | 최초의 GPT 모델 |
| **GPT-2** | 2019 | 15억 | 놀라운 텍스트 생성 능력 |
| **GPT-3** | 2020 | 1,750억 | 대규모 언어 이해 및 생성 |
| **GPT-4** | 2023 | 비공개 | 멀티모달(텍스트+이미지) 지원 |

> **파라미터(Parameter)**: 모델이 학습하는 변수의 개수. 일반적으로 파라미터가 많을수록 더 복잡한 패턴을 학습할 수 있습니다.

## Transformer의 장점

기존 RNN(Recurrent Neural Network) 방식 대비:
- ✅ **병렬 처리 가능**: 문장 전체를 동시에 처리하여 학습 속도가 빠름
- ✅ **장거리 의존성 파악**: 문장 앞부분과 뒷부분의 관계를 효과적으로 이해
- ✅ **확장성**: 데이터와 컴퓨팅 자원을 늘리면 성능이 지속적으로 향상

## 실제 활용

GPT 아키텍처는 다양한 분야에서 활용됩니다:
- **대화형 AI**: ChatGPT, Claude, Gemini
- **코드 생성**: GitHub Copilot, Cursor
- **번역**: DeepL, Google Translate
- **콘텐츠 생성**: 블로그 글, 마케팅 문구, 시나리오 작성

---

[← LLM 개요로 돌아가기](../LLM.md)

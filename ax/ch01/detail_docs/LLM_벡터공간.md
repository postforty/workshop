# LLM의 단어 벡터 공간

## 개요
LLM(대규모 언어 모델)은 단어의 의미를 숫자로 변환하여 다차원 공간에 배치합니다. 이를 통해 단어 간의 의미적 유사성을 계산하고 이해할 수 있습니다.

## 벡터 공간의 개념

### 다차원 공간
실제 언어 모델에서 단어 벡터는 **수백 차원의 공간**에 존재하지만, 시각화를 위해 2차원으로 축소하여 표현할 수 있습니다.

**단어의 벡터 표현 예시** (실제는 수백 차원이지만, 이해를 위해 5차원으로 단순화):
```
"사과(과일)"          = [0.2, 0.8, 0.1, 0.5, 0.3]
"apple"         = [0.3, 0.7, 0.2, 0.6, 0.4]  ← 사과(과일)와 유사한 값
"사과(용서)" = [0.9, 0.1, 0.8, 0.2, 0.7]  ← 사과(과일)와 매우 다른 값
"apology"       = [0.8, 0.2, 0.7, 0.3, 0.6]  ← 사과(apology)와 유사한 값
```

각 숫자는 단어의 특정 의미적 특성을 나타내며, 이 숫자들의 조합으로 단어의 의미를 표현합니다.

![벡터공간](../../materials/images/vector_space.png)

## 핵심 개념: 거리와 방향

벡터 공간에서 단어의 의미는 두 가지 핵심 요소로 표현됩니다:

### 📏 거리 (Distance)
단어 간의 **의미적 유사성**을 나타냅니다.
- **가까운 거리** = 의미가 비슷함 (예: "사과" ↔ "apple")
- **먼 거리** = 의미가 다름 (예: 과일 "사과" ↔ 용서 "사과")

### 🧭 방향 (Direction)
단어 간의 **관계성**을 나타냅니다.
- **성별 방향**: 왕 → 여왕 (남성 → 여성)
- **언어 방향**: apple → 사과 (영어 → 한국어)
- **지리 방향**: 한국 → 서울 (국가 → 수도)

## 주요 특징

### 1. 유사성 (Similarity)
"사과"와 "apple"은 의미론적으로 동일한 대상을 지칭하므로, 언어 모델의 벡터 공간에서 **서로 매우 가까운 위치**에 있을 가능성이 높습니다.

### 2. 언어별 클러스터 (Language Clusters)
- **한국어 클러스터**: "사과"는 다른 한국어 단어들(예: "배", "바나나", "맛있는")과 더 가까운 클러스터를 형성합니다.
- **영어 클러스터**: "apple"은 다른 영어 단어들(예: "pear", "banana", "delicious")과 더 가까운 클러스터를 형성합니다.

### 3. 공통 의미 축 (Semantic Axis)
이 두 클러스터 사이에 **"과일"이라는 공통 의미 축**이 존재하여 두 단어가 이 축을 따라 가깝게 위치하게 됩니다.

## 실제 적용

정확한 벡터 값은 모델마다 다르지만, 개념적인 위치 관계는 다음과 같은 원리로 작동합니다:

1. **의미가 비슷한 단어들은 가까이** 배치됩니다.
2. **의미가 다른 단어들은 멀리** 배치됩니다.
3. **관계가 있는 단어들은 특정 방향**으로 연결됩니다.

### 벡터 연산 예시

**관계 추론:**
- "왕" - "남자" + "여자" = "여왕" (성별 관계)
- "서울" - "한국" + "일본" = "도쿄" (수도 관계)

**언어 간 번역:**
- "apple" + (한국어 방향 벡터) = "사과"
- "thank you" + (한국어 방향 벡터) = "감사합니다"

벡터 공간에서 **언어 간 변환은 특정 방향으로의 이동**으로 표현됩니다. 예를 들어, 영어 단어에서 한국어 단어로 가는 "번역 벡터"가 존재하며, 이 벡터를 더하면 같은 의미의 한국어 단어에 도달하게 됩니다.

이러한 벡터 공간 덕분에 LLM은 단어의 의미를 수학적으로 계산하고 이해할 수 있으며, 다국어 번역도 가능합니다.

---

[← LLM 개요로 돌아가기](../LLM.md)
